[2025-06-22 10:20:04,702] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/v-shumingguo/add_tilelang/SeerAttention/seer_attn/modules/layernorm.py:510: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/home/v-shumingguo/add_tilelang/SeerAttention/seer_attn/modules/layernorm.py:569: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout, *args):
Namespace(model_name_or_path='SeerAttention/SeerAttention-Decode-Qwen3-4B-AttnGates', batch_size=4, limit=-1, data_dir='./data', data_name='aime24', split='test', max_tokens=32768, prompt_type='qwen-instruct', prompt_file_path='./prompts', surround_with_messages=True, use_few_shot=False, output_dir='./result_seer_sparse/SeerAttention-Decode-Qwen3-4B-AttnGates/aime24_bs4_token_budget_B4096_start0_blocksize64_seer_sparse', sparsity_method='token_budget', sliding_window_size=0, threshold=0, token_budget=4096, start_layer=0, block_size=64, rank=0, attention_implementation='seer_sparse', use_batch_exist=True, use_fused_kernel=True, profile_sparsity=True, run_id=0)
current eval model: SeerAttention/SeerAttention-Decode-Qwen3-4B-AttnGates
  0%|          | 0/30 [00:00<?, ?it/s]100%|██████████| 30/30 [00:00<00:00, 1239.45it/s]
<|im_start|>user
Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>assistant

Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 63550.06it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.57it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.58it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]
Some weights of SeerDecodingQwen3ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen3-4B and are newly initialized: ['model.layers.0.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.0.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.0.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.0.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.1.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.1.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.1.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.1.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.10.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.10.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.10.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.10.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.11.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.11.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.11.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.11.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.12.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.12.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.12.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.12.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.13.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.13.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.13.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.13.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.14.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.14.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.14.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.14.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.15.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.15.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.15.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.15.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.16.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.16.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.16.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.16.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.17.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.17.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.17.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.17.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.18.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.18.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.18.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.18.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.19.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.19.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.19.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.19.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.2.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.2.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.2.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.2.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.20.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.20.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.20.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.20.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.21.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.21.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.21.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.21.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.22.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.22.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.22.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.22.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.23.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.23.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.23.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.23.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.24.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.24.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.24.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.24.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.25.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.25.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.25.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.25.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.26.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.26.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.26.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.26.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.27.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.27.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.27.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.27.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.28.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.28.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.28.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.28.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.29.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.29.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.29.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.29.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.3.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.3.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.3.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.3.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.30.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.30.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.30.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.30.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.31.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.31.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.31.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.31.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.32.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.32.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.32.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.32.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.33.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.33.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.33.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.33.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.34.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.34.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.34.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.34.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.35.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.35.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.35.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.35.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.4.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.4.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.4.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.4.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.5.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.5.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.5.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.5.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.6.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.6.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.6.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.6.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.7.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.7.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.7.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.7.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.8.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.8.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.8.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.8.self_attn.attn_gate.attngate_qnorm.weight', 'model.layers.9.self_attn.attn_gate.attngate_knorm.weight', 'model.layers.9.self_attn.attn_gate.attngate_linear_k.weight', 'model.layers.9.self_attn.attn_gate.attngate_linear_q.weight', 'model.layers.9.self_attn.attn_gate.attngate_qnorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Attention gate weights loaded successfully.
start batch:  0
get output in batch:  0
finish batch:  0
start batch:  4
get output in batch:  4
finish batch:  4
start batch:  8
get output in batch:  8
finish batch:  8
start batch:  12
get output in batch:  12
finish batch:  12
start batch:  16
get output in batch:  16
finish batch:  16
start batch:  20
get output in batch:  20
finish batch:  20
start batch:  24
get output in batch:  24
finish batch:  24
start batch:  28
get output in batch:  28
finish batch:  28
llm generate done
Overall_sparsity:  0.7501137842330872
Successfully saved run0!
Namespace(model_name_or_path='SeerAttention/SeerAttention-Decode-Qwen3-4B-AttnGates', limit=-1, data_dir='./data', data_name='aime24', split='test', output_dir='./result_seer_sparse/SeerAttention-Decode-Qwen3-4B-AttnGates/aime24_bs4_token_budget_B4096_start0_blocksize64_seer_sparse', rank=0, total_run=1, profile_sparsity=True)
Successfully loaded run0!
Acc: 0.5
generate_lens:  [4437, 32612, 6521, 32612, 12283, 8587, 2985, 10574, 9357, 6553, 4524, 1517, 9715, 32633, 1447, 16288, 1841, 32548, 12972, 8437, 32566, 32566, 10432, 9870, 5798, 11459, 8437, 32601, 32352, 1735]
Acc: 0.5000
Max generate length: 32633
Average generate length: 14208.633333333333
Total time: 381.87014808257425min
Average time per token: 0.0008958641297487543
Overall_sparsity:  0.7501137842330872
Average sparsity at 16k: 0.74
Average sparsity at 32k: 0.87
Results saved to  ./result_seer_sparse/SeerAttention-Decode-Qwen3-4B-AttnGates/aime24_bs4_token_budget_B4096_start0_blocksize64_seer_sparse/overall_summary.txt

========================================
Starting task: aime24
Batch size: 4 | total_run: 1
Block size: 64

──────────────────────────────
Processing Task:aime24 | Block_size:64 | token_budget: window=0, budget=4096
Launching run 0 on GPU 0...
Run 0 on GPU 0 finished.
Successfully generated results for window=0, budget=4096

Completed: 64

Completed: aime24

 All tasks and configurations completed!
